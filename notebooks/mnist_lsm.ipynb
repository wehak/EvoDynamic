{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "leaky_custom_esn.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sixth-title",
        "outputId": "011e58df-6acb-4702-edc9-96c74077b2b4"
      },
      "source": [
        "!git clone https://github.com/SocratesNFR/EvoDynamic.git"
      ],
      "id": "sixth-title",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EvoDynamic'...\n",
            "remote: Enumerating objects: 898, done.\u001b[K\n",
            "remote: Counting objects: 100% (464/464), done.\u001b[K\n",
            "remote: Compressing objects: 100% (297/297), done.\u001b[K\n",
            "remote: Total 898 (delta 339), reused 278 (delta 163), pack-reused 434\u001b[K\n",
            "Receiving objects: 100% (898/898), 801.27 KiB | 526.00 KiB/s, done.\n",
            "Resolving deltas: 100% (633/633), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cnNDvC8TISI",
        "outputId": "370a774c-0059-4c66-ca93-a33c48539135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# import os\n",
        "# import sys\n",
        "# module_path = os.path.abspath(os.path.join('..'))\n",
        "# if module_path not in sys.path:\n",
        "#     sys.path.append(module_path)\n",
        "    \n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "#from project1.lib.module import function\n",
        "from EvoDynamic.evodynamic import experiment as experiment\n",
        "from EvoDynamic.evodynamic.connection import random as conn_random\n",
        "from EvoDynamic.evodynamic.connection import custom as conn_custom\n",
        "from EvoDynamic.evodynamic import connection as connection\n",
        "from EvoDynamic.evodynamic.cells import activation as act\n",
        "from EvoDynamic.evodynamic import utils as utils"
      ],
      "id": "4cnNDvC8TISI",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv1LSa2xV58G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6acde5f-4a9e-44ac-c122-720dcad58243"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train_num_images = x_train.shape[0]\n",
        "x_train_image_shape = x_train.shape[1:3]\n",
        "\n",
        "x_train = ((x_train / 255.0) > 0.5).astype(np.float64)\n",
        "x_train = x_train.reshape(x_train.shape[0],-1)\n",
        "x_train = np.transpose(x_train)\n",
        "\n",
        "x_test = ((x_test / 255.0) > 0.5).astype(np.float64)\n",
        "x_test = x_test.reshape(x_test.shape[0],-1)\n",
        "x_test = np.transpose(x_test)\n",
        "\n",
        "y_train_one_hot = np.zeros((y_train.max()+1, y_train.size))\n",
        "y_train_one_hot[y_train,np.arange(y_train.size)] = 1\n",
        "y_train = y_train_one_hot\n",
        "\n",
        "y_test_one_hot = np.zeros((y_test.max()+1, y_test.size))\n",
        "y_test_one_hot[y_test,np.arange(y_test.size)] = 1\n",
        "y_test = y_test_one_hot\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 100\n",
        "num_batches =  int(np.ceil(x_train_num_images / batch_size))\n",
        "width = 2*28*28\n",
        "input_size = 28*28\n",
        "output_layer_size = 10\n",
        "image_num_pixels = x_train_image_shape[0] * x_train_image_shape[1]\n",
        "threshold = 1.0\n",
        "potential_decay = 0.01\n",
        "\n",
        "exp = experiment.Experiment(input_start=0,input_delay=0,training_start=0,\n",
        "                            training_delay=0,reset_cells_after_train=True,\n",
        "                            batch_size=batch_size)\n",
        "\n",
        "\n",
        "input_lsm = exp.add_input(tf.float64, [input_size], \"input_esn\")\n",
        "desired_output = exp.add_desired_output(tf.float64, [output_layer_size], \"desired_output\")\n",
        "\n",
        "g_lsm = exp.add_group_cells(name=\"g_lsm\", amount=width)\n",
        "g_lsm_mem = g_lsm.add_real_state(state_name='g_lsm_mem')\n",
        "g_lsm_spike = g_lsm.add_binary_state(state_name='g_lsm_spike', init ='zeros')\n",
        "g_lsm_conn = conn_random.create_gaussian_matrix('g_lsm_conn',width, sparsity=0.95, is_sparse=True)\n",
        "# create_uniform_connection(name, from_group_amount, to_group_amount, sparsity=None, is_sparse=False)\n",
        "g_lsm_input_conn = conn_random.create_uniform_connection('g_lsm_input_conn', input_size, width, sparsity=0.9)\n",
        "\n",
        "\n",
        "\n",
        "exp.add_connection(\"input_conn\", connection.WeightedConnection(input_lsm,\n",
        "                                                              g_lsm_spike, act.integrate_and_fire,\n",
        "                                                              g_lsm_input_conn,\n",
        "                                                              fargs_list=[(g_lsm_mem,threshold,potential_decay)]))\n",
        "\n",
        "exp.add_connection(\"g_lsm_conn\",\n",
        "                   connection.WeightedConnection(g_lsm_spike,\n",
        "                                                 g_lsm_spike, act.integrate_and_fire,\n",
        "                                                 g_lsm_conn,\n",
        "                                                 fargs_list=[(g_lsm_mem,threshold,potential_decay)]))\n",
        "\n",
        "output_layer =  exp.add_group_cells(name=\"output_layer\", amount=output_layer_size)\n",
        "output_layer_real_state = output_layer.add_real_state(state_name='output_layer_real_state')\n",
        "\n",
        "lsm_output_conn = conn_random.create_xavier_connection(\"lsm_output_conn\", width, output_layer_size)\n",
        "exp.add_trainable_connection(\"output_conn\",\n",
        "                             connection.WeightedConnection(g_lsm_spike,\n",
        "                                                           output_layer_real_state,\n",
        "                                                           act.sigmoid,\n",
        "                                                           lsm_output_conn))\n",
        "\n",
        "c_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    logits=exp.trainable_connections[\"output_conn\"].output,\n",
        "    labels=desired_output,\n",
        "    axis=0))\n",
        "\n",
        "exp.set_training(c_loss,0.03)\n",
        "\n",
        "# Monitors are needed because \"reset_cells_after_train=True\"\n",
        "exp.add_monitor(\"output_layer\", \"output_layer_real_state\", timesteps=1)\n",
        "exp.add_monitor(\"g_lsm\", \"g_lsm_spike\", timesteps=1)\n",
        "\n",
        "exp.initialize_cells()"
      ],
      "id": "qv1LSa2xV58G",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LEARNING RATE 0.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "missing-asian",
        "outputId": "c0928dbb-9a28-478e-d726-be77484b8ef7"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(\"Epoch:\", epoch)\n",
        "  shuffled_indices = np.random.permutation(x_train_num_images)\n",
        "  batch_indices = np.split(shuffled_indices,\\\n",
        "                           np.arange(batch_size,x_train_num_images,batch_size))\n",
        "  for step, batch_idx in enumerate(batch_indices):\n",
        "    input_lsm_batch = x_train[:,batch_idx]\n",
        "    desired_output_batch = y_train[:,batch_idx]\n",
        "    feed_dict = {input_lsm: input_lsm_batch, desired_output: desired_output_batch}\n",
        "    exp.run_step(feed_dict=feed_dict)\n",
        "\n",
        "    prediction_batch = exp.get_monitor(\"output_layer\", \"output_layer_real_state\")[0,:,:]\n",
        "    accuracy_batch = np.sum(np.argmax(prediction_batch, axis=0) == np.argmax(desired_output_batch, axis=0)) / batch_size\n",
        "\n",
        "    utils.progressbar_loss_accu(step+1, num_batches, exp.training_loss, accuracy_batch)"
      ],
      "id": "missing-asian",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "[==================================================] 100.00%. Loss: 2.30259. Accuracy: 0.08000\n",
            "Epoch: 1\n",
            "[==================================================] 100.00%. Loss: 2.30259. Accuracy: 0.04000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}